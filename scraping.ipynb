{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9ef1123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from datetime import datetime\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edae5971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Navigating to https://repository.unhas.ac.id/view/divisions/statistika/...\n",
      "\n",
      "Processing Year: 2025\n",
      "  - Scraping [1/6]: Perbandingan Model Threshold Generalized Autoregressive Cond...\n",
      "  - Scraping [2/6]: PEMODELAN ROBUST MIXED GEOGRAPHICALLY AND TEMPORALLY WEIGHTE...\n",
      "  - Scraping [3/6]: Penggunaan Peta Kendali Generally Weighted Moving Average Pa...\n",
      "  - Scraping [4/6]: PENGGUNAAN METODE POSSIBILISTIC FUZZY C-MEANS UNTUK PENGELOM...\n",
      "  - Scraping [5/6]: ANALISIS KORELASI KANONIK MENGGUNAKAN MATRIKS VARIAN KOVARIA...\n",
      "  - Scraping [6/6]: PERAMALAN MODEL HYBRID METODE SEASONAL AUTOREGRESSIVE INTEGR...\n",
      "\n",
      "Processing Year: 2024\n",
      "  - Scraping [1/97]: Regresi Kuantil Elastic-Net dan Two-Step Robust Weighted Lea...\n",
      "  - Scraping [2/97]: MODEL REGRESI ROBUST IMPROVED GEOGRAPHICALLY AND TEMPORALLY ...\n",
      "  - Scraping [3/97]: Perbandingan Metode Random Forest dan Naive Bayes pada Klasi...\n",
      "  - Scraping [4/97]: Analisis Periode Kekeringan Meteorologis Berbasis Standardiz...\n",
      "  - Scraping [5/97]: PEMODELAN REGRESI ROBUST DENGAN ESTIMASI METHOD OF MOMENT DA...\n",
      "  - Scraping [6/97]: Analisis Perbandingan K-Nearest Neighbor Dan Support Vector ...\n",
      "  - Scraping [7/97]: PENERAPAN REGRESI ROBUST PADA FAKTOR-FAKTOR YANG MEMENGARUHI...\n",
      "  - Scraping [8/97]: Estimasi Model Regresi Nonparametrik Menggunakan Estimator K...\n",
      "  - Scraping [9/97]: PENERAPAN PETA KENDALI X ̅-S NEUTROSOPHIC PADA DATA SUHU KOT...\n",
      "  - Scraping [10/97]: ESTIMASI INTERVAL KEPERCAYAAN PARAMETER MODEL TINGKAT KEMISK...\n",
      "  - Scraping [11/97]: Aplikasi Metode Wavelet Thresholding Dengan Maximal Overlap ...\n",
      "  - Scraping [12/97]: Pemodelan Geographically Weighted Zero Inflated Negative Bin...\n",
      "  - Scraping [13/97]: PERFORMA PENGGUNAAN METODE SPATIAL EMPIRICAL BEST LINEAR UNB...\n",
      "  - Scraping [14/97]: Pemodelan Nonparametric Geographically and Temporally Weight...\n",
      "  - Scraping [15/97]: Pemodelan Robust Geographically and Temporally Weighted Auto...\n",
      "  - Scraping [16/97]: PERAMALAN CURAH HUJAN EKSTREM DI KOTA MAKASSAR MENGGUNAKAN M...\n",
      "  - Scraping [17/97]: Estimasi Model Gula Darah Pasien Diabetes Melalui Regresi Sp...\n",
      "  - Scraping [18/97]: PENDEKATAN MINIMUM VARIANCE QUADRATIC UNBIASED ESTIMATION DA...\n",
      "  - Scraping [19/97]: EVALUASI MODEL HYBRID CLUSTERING LARGE APPLICATIONS DAN FUZZ...\n",
      "  - Scraping [20/97]: MODEL REGRESI LOGISTIK SPARSE PRINCIPAL COMPONENT ANALYSIS P...\n",
      "  - Scraping [21/97]: PEMODELAN JACKKNIFE RIDGE MM-ESTIMATOR PADA DATA PRODUK DOME...\n",
      "  - Scraping [22/97]: Penentuan Cluster Terbaik Menggunakan Metode Fuzzy C-Means d...\n",
      "  - Scraping [23/97]: Pemodelan Regresi Cox Proportional Hazard Pada Data Ties Den...\n",
      "  - Scraping [24/97]: Estimasi Model Regresi Nonparametrik Multiprediktor Mengguna...\n",
      "  - Scraping [25/97]: Model Regresi Linear menggunakan Metode Bayesian pada Data P...\n",
      "  - Scraping [26/97]: Pemodelan Regresi Kuantil Dengan Estimator Spline Kubik Mult...\n",
      "  - Scraping [27/97]: Penerapan Metode Finite Mixture Partial Least Square dalam A...\n",
      "  - Scraping [28/97]: PEMODELAN DATA CURAH HUJAN MELALUI REGRESI NONPARAMETRIK MUL...\n",
      "  - Scraping [29/97]: APLIKASI PEGEL’S EKSPONENTIAL SMOOTHING MENGGUNAKAN OPTIMASI...\n",
      "  - Scraping [30/97]: Pemodelan Regresi Spasial dengan Pembobot Customize Berdasar...\n",
      "  - Scraping [31/97]: Pemodelan Geographically Weighted Poisson Regression Semipar...\n",
      "  - Scraping [32/97]: PENERAPAN PETA KENDALI POISSON PROGRESSIVE MEAN PADA DATA JU...\n",
      "  - Scraping [33/97]: PERAMALAN DENGAN HYBRID AUTOREGRESSIVE INTEGRATED MOVING AVE...\n",
      "  - Scraping [34/97]: ESTIMASI RISIKO INVESTASI SAHAM DENGAN CONDITIONAL VALUE AT ...\n",
      "  - Scraping [35/97]: PEMODELAN REGRESI LOGISTIK BINER BAYESIAN DENGAN METODE ADAP...\n",
      "  - Scraping [36/97]: Pemodelan Regresi Seemingly Unrelated Menggunakan Metode Max...\n",
      "  - Scraping [37/97]: PEMODELAN SECOND ORDER GENERALIZED STRUCTURAL COMPONENT ANAL...\n",
      "  - Scraping [38/97]: Pemodelan Geographically Weighted Bivariate Poisson Inverse ...\n",
      "  - Scraping [39/97]: PERBANDINGAN METODE HYBRID SSA-SVR DAN HYBRID SSA-ARIMA DALA...\n",
      "  - Scraping [40/97]: PEMODELAN GEOMETRIC BROWNIAN MOTION DAN VALUE AT RISK DALAM ...\n",
      "  - Scraping [41/97]: PEMODELAN REGRESI ROBUST MENGGUNAKAN LEAST MEDIAN OF SQUARES...\n",
      "  - Scraping [42/97]: PENGUJIAN KESAMAAN VEKTOR PARAMETER REGRESI LOGISTIK BINER P...\n",
      "  - Scraping [43/97]: PENERAPAN METODE RESIDUL BOOTSTRAP PADA SPATIAL AUTOREGRESSI...\n",
      "  - Scraping [44/97]: Model Data Kemiskinan di Provinsi Papua dengan Regresi Semip...\n",
      "  - Scraping [45/97]: Perbandingan Akurasi Peramalan Metode Autoregressive Integra...\n",
      "  - Scraping [46/97]: PEMODELAN ROBUST GEOGRAPHICALLY WEIGHTED BINARY LOGISTIC REG...\n",
      "  - Scraping [47/97]: PENERAPAN PETA KENDALI EXPONENTIALLY WEIGHTED MOVING AVERAGE...\n",
      "  - Scraping [48/97]: Peramalan dengan Hybrid Exponential Smoothing - Neural Netwo...\n",
      "  - Scraping [49/97]: Penggunaan Metode Gaussian Kernel Fuzzy C-Means untuk Pengel...\n",
      "  - Scraping [50/97]: Pemodelan Regresi Semiparametrik Smoothing Spline pada Data ...\n",
      "  - Scraping [51/97]: Perbandingan K-Means dan K-Medoids pada Time Series-Based Cl...\n",
      "  - Scraping [52/97]: Pemodelan Robust Spatial Autoregressive dengan Metode MM-Est...\n",
      "  - Scraping [53/97]: ALGORITMA K-NEAREST NEIGHBOR SPASIO TEMPORAL DENSITY BASED C...\n",
      "  - Scraping [54/97]: PEMODELAN GAUSSIAN COPULA MARGINAL REGRESSION (GCMR) UNTUK M...\n",
      "  - Scraping [55/97]: PENGGUNAAN METODE WAVELET THRESHOLDING DENGAN MAXIMAL OVERLA...\n",
      "  - Scraping [56/97]: PENGGUNAAN PETA KENDALI EXTENDED EXPONENTIALLY WEIGHTED MOVI...\n",
      "  - Scraping [57/97]: Penerapan Peta Kendali Arcsin Exponentially Weighted Moving ...\n",
      "  - Scraping [58/97]: Penggunaan Single Index Model dalam Optimalisasi Portofolio ...\n",
      "  - Scraping [59/97]: Pemodelan Regresi Nonparametrik Poisson dengan Estimator Pen...\n",
      "  - Scraping [60/97]: Penggunaan Peta Kendali Homogeneously Weighted Moving Averag...\n",
      "  - Scraping [61/97]: Pemodelan Regresi Nonparametrik Birespon Bootstrap Aggregati...\n",
      "  - Scraping [62/97]: PERFORMA MODEL REGRESI KOMPONEN UTAMA SPARSE PADA DATA ANGKA...\n",
      "  - Scraping [63/97]: Pengendalian Kualitas Proses Produksi Melalui Penerapan Peta...\n",
      "  - Scraping [64/97]: Pemodelan Generalized Poisson Regression dengan Estimasi Mod...\n",
      "  - Scraping [65/97]: Pemodelan Geographically and Temporally Weighted Regression ...\n",
      "  - Scraping [66/97]: PERBANDINGAN PETA KENDALI POISSON DOUBLE PROGRESSIVE MEAN DA...\n",
      "  - Scraping [67/97]: Analysis of meteorological drought periods based on the Stan...\n",
      "  - Scraping [68/97]: PEMODELAN DATA WAKTU ANTAR KEJADIAN GEMPA BUMI YANG MENGANDU...\n",
      "  - Scraping [69/97]: Pemodelan Regresi Zero Inflated Poisson Inverse Gaussian Pad...\n",
      "  - Scraping [70/97]: Cluster analysis of rainfall patterns in Mamminasata: Valida...\n",
      "  - Scraping [71/97]: Estimasi Indeks Harga Saham Gabungan Berdasarkan Model Regre...\n",
      "  - Scraping [72/97]: Pemodelan Bivariate Poisson Log-Normal Regression Pada Jumla...\n",
      "  - Scraping [73/97]: PENERAPAN BAGAN KENDALI NON PARAMETRIK DENGAN MENGGUKANAN ME...\n",
      "  - Scraping [74/97]: PENERAPAN MODEL MARKOV SWITCHING AUTOREGRESSIVE DALAM PERAMA...\n",
      "  - Scraping [75/97]: Perbandingan Peta Kendali Mixed Exponentially Weighted Movin...\n",
      "  - Scraping [76/97]: Pemodelan Regresi Nonparametrik dengan Pendekatan Complete F...\n",
      "  - Scraping [77/97]: Estimasi Paramater Model Panel Vector Autoregressive Dengan ...\n",
      "  - Scraping [78/97]: Pemodelan Hybrid Generalized Space Time Autoregressive dan S...\n",
      "  - Scraping [79/97]: Pemodelan Geographically Weighted Bivariate Poisson Inverse ...\n",
      "  - Scraping [80/97]: PERBANDINGAN GENERALIZED ESTIMATING EQUATIONS DAN GENERALIZE...\n",
      "  - Scraping [81/97]: Penerapan Least Absolute Shrinkage and Selection Operator pa...\n",
      "  - Scraping [82/97]: Pemodelan Statistical Downscaling Menggunakan Robust Jackkni...\n",
      "  - Scraping [83/97]: PENERAPAN PETA KENDALI NEUTROSOPHIC MOVING AVERAGE PADA DATA...\n",
      "  - Scraping [84/97]: PERBANDINGAN ESTIMASI VALUE AT RISK MENGGUNAKAN MULTIVARIAT ...\n",
      "  - Scraping [85/97]: PEMODELAN REGRESI HURDLE BINOMIAL NEGATIF DAN HURDLE CONWAY ...\n",
      "  - Scraping [86/97]: Pemodelan Modified Spatial Durbin Model dengan Pendekatan Me...\n",
      "  - Scraping [87/97]: PENGEMBANGAN BAGAN KENDALI TRIPLE HOMOGENEOUSLY WEIGHTED MOV...\n",
      "  - Scraping [88/97]: Analisis Sentimen MyPertamina Menggunakan Klasifikasi Naive ...\n",
      "  - Scraping [89/97]: Pemodelan Eigenvector Spatial Filtering dan Random Effect Ei...\n",
      "  - Scraping [90/97]: Perbandingan Kinerja Metode Naïve Bayes dan Correlated Naïve...\n",
      "  - Scraping [91/97]: Analisis Faktor Pengaruh Jumlah Kasus HIV dan AIDS di Indone...\n",
      "  - Scraping [92/97]: REGRESI KUANTIL DENGAN PENALTI GROUP LASSO UNTUK MEMODELKAN ...\n",
      "  - Scraping [93/97]: PENERAPAN GRAVITATIONAL SEARCH ALGORITHM PADA FUZZY GEOGRAPH...\n",
      "  - Scraping [94/97]: PENDEKATAN REGRESI KUANTIL SPLINE DALAM STATISTICAL DOWNSCAL...\n",
      "  - Scraping [95/97]: MODEL REGRESI DATA PANEL DENGAN STANDARD ERROR BERDASARKAN H...\n",
      "  - Scraping [96/97]: PEMODELAN REGRESI ELASTIC NET ROBUST MM-ESTIMATION PADA DATA...\n",
      "  - Scraping [97/97]: PERBANDINGAN METODE MINIMUM COVARIANCE DETERMINANT DAN MINIM...\n",
      "\n",
      "Processing Year: 2023\n",
      "  - Scraping [1/68]: Penggunaan Metode Improved Square Root Transformation pada P...\n",
      "  - Scraping [2/68]: ANALISIS SENTIMEN KEBENCANAAN DI INDONESIA MENGGUNAKAN SUPPO...\n",
      "  - Scraping [3/68]: Perbandingan Kinerja K-Medoids dan Density Based Spatial Clu...\n",
      "  - Scraping [4/68]: PEMODELAN ANALISIS KOMPONEN UTAMA ROBUST MENGGUNAKAN METODE ...\n",
      "  - Scraping [5/68]: Pemodelan Regresi Logistik Ordinal dengan Estimator Spline T...\n",
      "  - Scraping [6/68]: Pemodelan Indeks Saham LQ45 Dengan Fungsi Kernel Triweight D...\n",
      "  - Scraping [7/68]: PERBANDINGAN KINERJA SUPPORT VECTOR MACHINE DENGAN OPTIMASI ...\n",
      "  - Scraping [8/68]: Peta Kendali Mixed Cumulative Sum – Exponentially Weighted M...\n",
      "  - Scraping [9/68]: PERBANDINGAN METODE ARTIFICIAL NEURAL NETWORK BACKPROPAGATIO...\n",
      "  - Scraping [10/68]: PEMODELAN REGRESI CONWAY MAXWELL POISSON DAN REGRESI ZERO IN...\n",
      "  - Scraping [11/68]: Analisis Regresi Komponen Utama Robust Dengan Metode Minimum...\n",
      "  - Scraping [12/68]: Penggunaan Seleksi Fitur Query Expansion Ranking dan Genetic...\n",
      "  - Scraping [13/68]: METODE KONSTRUKSI SKEMA ASOSIASI RIGHT ANGULAR DAN CIRCULAR ...\n",
      "  - Scraping [14/68]: PENERAPAN SYNTHETIC MINORITY OVERSAMPLING TECHNIQUE PADA SEN...\n",
      "  - Scraping [15/68]: Perbandingan Metode Klasifikasi Algoritma Naïve Bayes Tanpa ...\n",
      "  - Scraping [16/68]: Pemodelan Linearized Ridge MM-Estimator Pada Data Angka Kema...\n",
      "  - Scraping [17/68]: Penerapan Metode K-Medoids Menggunakan Within Cluster Variat...\n",
      "  - Scraping [18/68]: Perbandingan kinerja peta kendali laney p dan peta kendali p...\n",
      "  - Scraping [19/68]: Penerapan Combine Sampling Pada Analisis Sentimen Menggunaka...\n",
      "  - Scraping [20/68]: Penerapan Regresi Nonparametrik Birespon dengan Penaksir B-S...\n",
      "  - Scraping [21/68]: PENGGUNAAN METODE SINGULAR SPECTRUM ANALYSIS DALAM PERAMALAN...\n",
      "  - Scraping [22/68]: PEMODELAN REGRESI SPLINE HIERARKI DENGAN METODE GENERALIZED ...\n",
      "  - Scraping [23/68]: Pemodelan Geographically Weighted Regression dengan Fungsi P...\n",
      "  - Scraping [24/68]: Penerapan Peta Kendali Link Relative Cumulative Sum Dan Link...\n",
      "  - Scraping [25/68]: Perbandingan Hasil Peramalan Metode Fuzzy Time Series Lee da...\n",
      "  - Scraping [26/68]: Bagan Kendali Menggunakan Metode Nonparametrik Exponentially...\n",
      "  - Scraping [27/68]: Peramalan Jumlah Konsumsi Bahan Bakar Minyak Jenis Pertalite...\n",
      "  - Scraping [28/68]: Pemodelan Small Area Estimation untuk Estimasi Pengeluaran p...\n",
      "  - Scraping [29/68]: Analisis Survival dengan Model Regresi Cox Stratified pada P...\n",
      "  - Scraping [30/68]: Penerapan Peta Kendali Nonparametrik Cumulative - Sum Berdas...\n",
      "  - Scraping [31/68]: Pemodelan Geographically Weighted Panel Regression Pada Data...\n",
      "  - Scraping [32/68]: ANALISIS SEBARAN SPASIAL VEGETASI MANGROVE PADA KAWASAN HUTA...\n",
      "  - Scraping [33/68]: Analisis Kejadian Curah Hujan Ekstrem di Kota Makassar Mengg...\n",
      "  - Scraping [34/68]: Estimasi Value at Risk dalam Investasi Saham Perusahaan Sekt...\n",
      "  - Scraping [35/68]: Peta Kendali Demerit Untuk Data Autokorelasi = Control Chart...\n",
      "  - Scraping [36/68]: POTENSI EKSTRAK DAUN NIPAH (Nypa fruticans) SEBAGAI ANTIKANK...\n",
      "  - Scraping [37/68]: Penggunaan Bagan Kendali Fuzzy Exponentially Weighted Moving...\n",
      "  - Scraping [38/68]: PEMODELAN GEOGRAPHICALLY WEIGHTED NEGATIVE BINOMIAL REGRESSI...\n",
      "  - Scraping [39/68]: Peramalan Cryptocurrency Menggunakan Model Fuzzy Time Series...\n",
      "  - Scraping [40/68]: Penerapan Metode Exhaustive Chi-Square Automatic Interaction...\n",
      "  - Scraping [41/68]: Perbandingan Bagan Kendali Multivariate Cumulative Sum 1 dan...\n",
      "  - Scraping [42/68]: Pemodelan Regresi Binomial Negatif Menggunakan Estimator Jac...\n",
      "  - Scraping [43/68]: Penggunaan Metode Copula Gaussian untuk Menentukan Nilai Val...\n",
      "  - Scraping [44/68]: Penerapan Model Fungsi Transfer Multi Input pada Peramalan T...\n",
      "  - Scraping [45/68]: PENDUGAAN SELANG KEPERCAYAAN EKSAK CLOPPER-PEARSON PADA PROP...\n",
      "  - Scraping [46/68]: Penggunaan Bagan Kendali Hybrid Exponentially Weighted Movin...\n",
      "  - Scraping [47/68]: Pemodelan Semiparametric Geographically Weighted Regression ...\n",
      "  - Scraping [48/68]: Penerapan Bagan Kendali Modified Cumulative Sum dalam Mendet...\n",
      "  - Scraping [49/68]: RANCANGAN CROSS OVER YANG MENGANDUNG OUTLIER DENGAN ESTIMASI...\n",
      "  - Scraping [50/68]: PERBANDINGAN K-MEANS CLUSTER DENGAN DENSITY-BASED SPATIAL CL...\n",
      "  - Scraping [51/68]: PETA KENDALI ZERO INFLATED GENERALIZED POISSON MODIFIKASI EX...\n",
      "  - Scraping [52/68]: Pemodelan Geographically Weighted Regression dengan Pendekat...\n",
      "  - Scraping [53/68]: ANALISIS RESPONSE BASED UNITS SEGMENTATION PARTIAL LEAST SQU...\n",
      "  - Scraping [54/68]: Penerapan Algoritma Fuzzy Subtractive Clustering untuk Penge...\n",
      "  - Scraping [55/68]: Estimasi Parameter Model Persamaan Simultan Menggunakan Meto...\n",
      "  - Scraping [56/68]: Pemodelan Regresi Data Panel Tidak Seimbang Menggunakan Meto...\n",
      "  - Scraping [57/68]: Performa Peta Kendali Double Exponentially Weighted Moving A...\n",
      "  - Scraping [58/68]: PEMODELAN ROBUST SPATIAL DURBIN MODEL PADA DATA KASUS TUBERK...\n",
      "  - Scraping [59/68]: PENERAPAN METODE ROBUST KRIGING PADA DATA CURAH HUJAN WILAYA...\n",
      "  - Scraping [60/68]: PENERAPAN ADAPTIVE NEURO FUZZY INFERENCE SYSTEM (ANFIS) DALA...\n",
      "  - Scraping [61/68]: Pendugaan Peluang Kenaikan Harga Saham Menggunakan Selang Ke...\n",
      "  - Scraping [62/68]: Perbandingan Metode Autoregressive Integrated Moving Average...\n",
      "  - Scraping [63/68]: PEMODELAN REGRESI DATA PANEL TIDAK SEIMBANG MENGGUNAKAN METO...\n",
      "  - Scraping [64/68]: ESTIMASI PARAMETER REGRESI RIDGE ROBUST PADA DATA PROFIL KES...\n",
      "  - Scraping [65/68]: Estimasi koefisien regresi logistik biner dengan metode leas...\n",
      "  - Scraping [66/68]: ANALISIS PERBANDINGAN ALGORITMA NAÏVE BAYES DAN SUPPORT VECT...\n",
      "  - Scraping [67/68]: Pemodelan Regresi Semiparametrik Birespon dengan Estimator S...\n",
      "  - Scraping [68/68]: Analisis Sentimen Kurikulum Merdeka Menggunakan Klasifikasi ...\n",
      "\n",
      "Processing Year: 2022\n",
      "  - Scraping [1/51]: Monitoring Mean Process pada Kualitas Air PDAM Tirta Jeneber...\n",
      "  - Scraping [2/51]: Pemodelan Regresi Data Panel Dinamis dengan Pendekatan Gener...\n",
      "  - Scraping [3/51]: PEMODELAN GEOGRAPHICALLY WEIGHTED ZERO INFLATED POISSON REGR...\n",
      "  - Scraping [4/51]: Performa Bagan Kendali Nonparametrik Exponentially Weighted ...\n",
      "  - Scraping [5/51]: KINERJA MODEL EXPONENTIAL DAN INTEGRATED GARCH PADA ANALISIS...\n",
      "  - Scraping [6/51]: Aplikasi Metode Improved Chi Square Automatic Interaction De...\n",
      "  - Scraping [7/51]: Pemodelan Geographically Weighted LASSO Untuk Data Yang Meng...\n",
      "  - Scraping [8/51]: Penggunaan Metode Regresi Kuantil Komponen Utama pada Data A...\n",
      "  - Scraping [9/51]: Penggunaan Metode Robust Scale Untuk Mengatasi Outlier Pada ...\n",
      "  - Scraping [10/51]: ESTIMASI INDEKS HARGA SAHAM GABUNGAN BERDASARKAN MODEL REGRE...\n",
      "  - Scraping [11/51]: Pemodelan Semiparametrik dengan Koefisien Bervariasi pada Da...\n",
      "  - Scraping [12/51]: ANALISIS FAKTOR-FAKTOR YANG MEMPENGARUHI INVESTASI ASING LAN...\n",
      "  - Scraping [13/51]: Peta Kendali c Berdasarkan Metode Modifikasi Transformasi Ak...\n",
      "  - Scraping [14/51]: PENERAPAN MODEL REGRESI LOGISTIK LEAST ABSOLUTE SHRINKAGE AN...\n",
      "  - Scraping [15/51]: Analisis Rantai Markov Terboboti untuk Memprediksi Nilai Eks...\n",
      "  - Scraping [16/51]: ESTIMASI INTERVAL KEPERCAYAAN PARAMETER REGRESI DALAM MODEL ...\n",
      "  - Scraping [17/51]: Perbandingan Hasil Peramalan Dengan Metode Fuzzy Time Series...\n",
      "  - Scraping [18/51]: Penerapan Estimator Priestley-Chao dengan Fungsi Kernel Gaus...\n",
      "  - Scraping [19/51]: Kinerja Peta kendali Triple Exponentially Weighted Moving Av...\n",
      "  - Scraping [20/51]: Analisis Data Longitudinal dengan Korelasi Kanonik Multivari...\n",
      "  - Scraping [21/51]: ESTIMASI PARAMETER MODEL MIXED GEOGRAPHICALLY WEIGHTED REGRE...\n",
      "  - Scraping [22/51]: MODEL MULTILEVEL SPLINE LINEAR TRUNCATED DALAM REGRESI NONPA...\n",
      "  - Scraping [23/51]: ESTIMASI DAN PERAMALAN JUMLAH KASUS COVID-19 DI SULAWESI SEL...\n",
      "  - Scraping [24/51]: Estimasi Model Regresi Nonparametrik Menggunakan Estimator N...\n",
      "  - Scraping [25/51]: Penerapan Metode Copula Archimedean dalam Analisis Value at ...\n",
      "  - Scraping [26/51]: ESTIMASI PARAMETER MODEL DATA PANEL DINAMIS MENGGUNAKAN METO...\n",
      "  - Scraping [27/51]: INTERPOLASI SPASIAL ORDINARY COKRIGING DENGAN SEMIVARIOGRAM ...\n",
      "  - Scraping [28/51]: PENERAPAN REGRESI PANEL DINAMIS DENGAN PENDEKATAN GENERALIZE...\n",
      "  - Scraping [29/51]: PERBANDINGAN KINERJA METODE MESIN VEKTOR PENDUKUNG TANPA DAN...\n",
      "  - Scraping [30/51]: Penggunaan Peta Kendali Poisson Exponentially Weighted Movin...\n",
      "  - Scraping [31/51]: Pemodelan Jumlah Kematian Ibu dan Bayi di Provinsi Sulawesi ...\n",
      "  - Scraping [32/51]: Analisis Regresi Data Panel Dengan Model Efek Umum, Model Ef...\n",
      "  - Scraping [33/51]: Pemodelan Regresi Binomial Negatif Bivariat pada Data yang M...\n",
      "  - Scraping [34/51]: PENERAPAN ANALISIS KOMPONEN UTAMA PADA ALGORITMA LONG SHORT ...\n",
      "  - Scraping [35/51]: Pemodelan Data Longitudinal Multikolinearitas dengan Princip...\n",
      "  - Scraping [36/51]: Perbandingan Akurasi Peramalan Jumlah Penumpang Pesawat Band...\n",
      "  - Scraping [37/51]: Penerapan Fuzzy Inference System dengan Metode Tsukamoto unt...\n",
      "  - Scraping [38/51]: PERBANDINGAN KINERJA METODE GENETIC ALGORITHM K-NEAREST NEIG...\n",
      "  - Scraping [39/51]: Perbandingan Estimasi LASSO Dan LASSO Least Trimmed Squares ...\n",
      "  - Scraping [40/51]: ANALISIS SENTIMEN KEBIJAKAN MERDEKA BELAJAR KAMPUS MERDEKA M...\n",
      "  - Scraping [41/51]: ESTIMASI CONDITIONAL VALUE AT RISK DENGAN METODE COPULA ALI-...\n",
      "  - Scraping [42/51]: Pemodelan regresi Conway Maxwell Poisson untuk mengatasi pel...\n",
      "  - Scraping [43/51]: Estimasi Nilai Conditional Value At Risk Pada Harga Penutupa...\n",
      "  - Scraping [44/51]: PENGGUNAAN METODE XGBOOST UNTUK KLASIFIKASI STATUS OBESITAS ...\n",
      "  - Scraping [45/51]: ESTIMASI MODEL REGRESI LOGISTIK BINER NONPARAMETRIK DENGAN P...\n",
      "  - Scraping [46/51]: MODEL REGRESI LOGISTIK ORDINAL MULTILEVEL PADA TINGKAT PREDI...\n",
      "  - Scraping [47/51]: An Estimator For Simultaneous Equation Model Using Two Stage...\n",
      "  - Scraping [48/51]: PENERAPAN MODEL EXPONENTIAL GENERALIZED AUTOREGRESSIVE CONDI...\n",
      "  - Scraping [49/51]: PERBANDINGAN KINERJA BAGAN KENDALI MULTIVARIATE EXPONENTIALL...\n",
      "  - Scraping [50/51]: PEMODELAN PERTUMBUHAN EKONOMI DENGAN METODE TWO STAGE LEAST ...\n",
      "  - Scraping [51/51]: ESTIMASI MODEL REGRESI SPLINE BIRESPON DENGAN FUNGSI PENALTY...\n",
      "\n",
      "Processing Year: 2021\n",
      "  - Scraping [1/9]: PENGGUNAAN METODE REGRESI AKAR LATEN ROBUST PADA DATA YANG M...\n",
      "  - Scraping [2/9]: Perbandingan Efektivitas Peta Kendali Decision On Belief dan...\n",
      "  - Scraping [3/9]: Analisis Data Panel dengan Model Regresi Logistik Biner = Pa...\n",
      "  - Scraping [4/9]: ANALISIS NILAI RISIKO PADA INDEKS HARGA SAHAM GABUNGAN MENGG...\n",
      "  - Scraping [5/9]: PERFORMA PETA KENDALI AUTOREGRESSIVE INTEGRATED MOVING AVERA...\n",
      "  - Scraping [6/9]: PEMODELAN ANALISIS KOMPONEN UTAMA REGRESI TERBOBOTI SECARA G...\n",
      "  - Scraping [7/9]: PENGUJIAN HIPOTESIS MODEL REGRESI NONPARAMETRIK SPLINE TRUNC...\n",
      "  - Scraping [8/9]: Estimasi Parameter Fungsi Intensitas Bersyarat Model Point P...\n",
      "  - Scraping [9/9]: Estimasi Model Regresi Nonparametrik Poisson Diperumum Mengg...\n",
      "\n",
      "Processing Year: 2020\n",
      "  - Scraping [1/2]: PERFORMA MODEL STATISTICAL DOWNSCALING DENGAN PEUBAH DUMMY B...\n",
      "  - Scraping [2/2]: Peramalan Jumlah Penumpang Kapal Laut Menggunakan Metode Fuz...\n",
      "\n",
      "Processing Year: 2019\n",
      "  - Scraping [1/2]: ESTIMATOR KURVA PRIESTLEY CHAO MENGGUNAKAN FUNGSI KERNEL TRI...\n",
      "  - Scraping [2/2]: ESTIMASI PARAMETER DISTRIBUSI T MENGGUNAKAN ALGORITMA EXPECT...\n",
      "\n",
      "Processing Year: 2017\n",
      "  - Scraping [1/5]: PEMILIHAN VARIABEL PREDIKTOR DENGAN METODE STEPWISE PADA MOD...\n",
      "  - Scraping [2/5]: MODEL PERSAMAAN STRUKTURAL DENGAN MENGGUNAKAN PARTIAL LEAST ...\n",
      "  - Scraping [3/5]: MODEL FRAILTY SPASIAL SURVIVAL UNTUK PEMETAAN PENYAKIT DEMAM...\n",
      "  - Scraping [4/5]: ESTIMASI DATA HILANG PADA RANCANGAN CROSS-OVER MENGGUNAKAN M...\n",
      "  - Scraping [5/5]: PENGGUNAAN MIXED GEOGRAPHICALLY WEIGHTED REGRESSION PADA DAT...\n",
      "\n",
      "Processing Year: 2008\n",
      "  - Scraping [1/1]: PENGGUNAAN DATA KELANGSUNGAN HIDUP UNIVARIAT DENGAN PENDEKAT...\n",
      "\n",
      "Processing Year: 2006\n",
      "  - Scraping [1/2]: PENGGUNAAN WAVELET HAAR DALAM MENGANALISIS DATA RUNTUN WAKTU...\n",
      "  - Scraping [2/2]: ANALISIS DATA SURVIVAL MENGGUNAKAN REGRESI LOGISTIK DAN KURV...\n",
      "\n",
      "Processing Year: 2005\n",
      "  - Scraping [1/1]: PENGENDALIAN KUALITAS STATISTIKA RUNTUN WAKTU DENGAN METODE ...\n",
      "\n",
      "Processing Year: 2003\n",
      "  - Scraping [1/3]: KALMAN FILTERING DALAM MEMPREDIKSI LIME SATURATION FACTOR (L...\n",
      "  - Scraping [2/3]: ANALISIS VARIANSI DUA ARAH DAN PEMBANDING GANDA DAN SIFAT RU...\n",
      "  - Scraping [3/3]: PENGELOMPOKAN POPULASI BERDASARKAN WALD-ANDERSON DENGAN MATR...\n",
      "\n",
      "Processing Year: 2002\n",
      "  - Scraping [1/1]: PENENTUAN KORELASI PERINGKAT T-KENDALL DATA KONTINGENSI...\n",
      "\n",
      "Processing Year: NULL\n",
      "  - Scraping [1/4]: A novel hybrid CLARA and fuzzy time series Markov chain mode...\n",
      "  - Scraping [2/4]: Return Levels on Stationary Extreme Rainfall Series: A Compa...\n",
      "  - Scraping [3/4]: Assessing SPI and SPEI for drought forecasting through the p...\n",
      "  - Scraping [4/4]: Mapping Meteorological Drought Periods in South Sulawesi Usi...\n",
      "\n",
      "✅ Scraping complete. Data has been saved to 'output/unhas_repository_20250730_004812.json'.\n"
     ]
    }
   ],
   "source": [
    "def scrape_repository():\n",
    "    \"\"\"\n",
    "    Scrapes thesis data from the UNHAS Statistics repository.\n",
    "    \"\"\"\n",
    "    # Automatically install and set up the ChromeDriver\n",
    "    service = ChromeService(ChromeDriverManager().install())\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\") # Optional: run in background\n",
    "    options.add_argument(\"--log-level=3\") # Suppress console logs\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "    base_url = \"https://repository.unhas.ac.id/view/divisions/statistika/\"\n",
    "    print(f\"Navigating to {base_url}...\")\n",
    "    driver.get(base_url)\n",
    "    time.sleep(3) \n",
    "\n",
    "    repository_data = {}\n",
    "\n",
    "    # Find all year links on the main page to avoid stale elements\n",
    "    year_elements = driver.find_elements(By.XPATH, \"/html/body/div[1]/div/div[2]/div/ul/li/a\")\n",
    "    year_links = [(elem.text, elem.get_attribute('href')) for elem in year_elements]\n",
    "\n",
    "    def get_element_text_or_none(driver, xpath):\n",
    "        \"\"\"Safely gets text from an element, returning None if not found.\"\"\"\n",
    "        try:\n",
    "            return driver.find_element(By.XPATH, xpath).text.strip()\n",
    "        except NoSuchElementException:\n",
    "            return None\n",
    "\n",
    "    # Loop 1: Iterate through each year\n",
    "    for year_text, year_url in year_links:\n",
    "        print(f\"\\nProcessing Year: {year_text}\")\n",
    "        repository_data[year_text] = {}\n",
    "        driver.get(year_url)\n",
    "        time.sleep(2)\n",
    "\n",
    "        thesis_urls = []\n",
    "        thesis_index = 1\n",
    "        # Loop 2: Find all thesis links for the current year\n",
    "        while True:\n",
    "            try:\n",
    "                xpath = f\"/html/body/div[1]/div/div[2]/div[2]/p[{thesis_index}]/a\"\n",
    "                thesis_link_element = driver.find_element(By.XPATH, xpath)\n",
    "                thesis_urls.append(thesis_link_element.get_attribute('href'))\n",
    "                thesis_index += 1\n",
    "            except NoSuchElementException:\n",
    "                break # Exit loop when no more thesis links are found\n",
    "        \n",
    "        # Loop 3: Visit each thesis page and scrape data\n",
    "        for i, thesis_url in enumerate(thesis_urls):\n",
    "            driver.get(thesis_url)\n",
    "            time.sleep(1)\n",
    "\n",
    "            title = get_element_text_or_none(driver, '//*[@id=\"page-title\"]')\n",
    "            if not title:\n",
    "                print(f\"  - Skipping entry {i+1}/{len(thesis_urls)} (Title not found)\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"  - Scraping [{i+1}/{len(thesis_urls)}]: {title[:60]}...\")\n",
    "\n",
    "            # Scrape all required details\n",
    "            thesis_details = {\n",
    "                \"author\": get_element_text_or_none(driver, \"/html/body/div[1]/div/div[2]/div/div[4]/p/span\"),\n",
    "                \"abstract\": get_element_text_or_none(driver, \"/html/body/div[1]/div/div[2]/div/div[4]/div[3]/p\"),\n",
    "                \"item_type\": get_element_text_or_none(driver, \"/html/body/div[1]/div/div[2]/div/div[4]/table/tbody/tr[1]/td\"),\n",
    "                \"date_deposited\": get_element_text_or_none(driver, \"/html/body/div[1]/div/div[2]/div/div[4]/table/tbody/tr[5]/td\"),\n",
    "                \"last_deposited\": get_element_text_or_none(driver, \"/html/body/div[1]/div/div[2]/div/div[4]/table/tbody/tr[6]/td\"),\n",
    "                \"url\": thesis_url\n",
    "            }\n",
    "            \n",
    "            repository_data[year_text][title] = thesis_details\n",
    "\n",
    "    # Save the final data structure to a JSON file\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_filename = f'output/unhas_repository_{timestamp}.json'\n",
    "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(repository_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"\\n✅ Scraping complete. Data has been saved to '{output_filename}'.\")\n",
    "    driver.quit()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    scrape_repository()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f667057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 252 items to classify. Starting process in batches of 20...\n",
      "  - Processing batch 1/13...\n",
      "    - Warning: API call failed on attempt 1. Error: Expecting value: line 1 column 1 (char 0)\n",
      "    - Warning: API call failed on attempt 2. Error: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 139\u001b[39m\n\u001b[32m    135\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✅ Classification complete! Results saved to \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUTPUT_FILENAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[43mclassify_theses\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 108\u001b[39m, in \u001b[36mclassify_theses\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(retries):\n\u001b[32m    107\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m         response = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m         classifications = json.loads(response.text)\n\u001b[32m    110\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m    - Batch successfully processed by API.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Kuliah\\Skripsi\\unhas-statistics-theses-scraping\\.venv\\Lib\\site-packages\\google\\generativeai\\generative_models.py:331\u001b[39m, in \u001b[36mGenerativeModel.generate_content\u001b[39m\u001b[34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[39m\n\u001b[32m    329\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types.GenerateContentResponse.from_iterator(iterator)\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m         response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    335\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types.GenerateContentResponse.from_response(response)\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m google.api_core.exceptions.InvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Kuliah\\Skripsi\\unhas-statistics-theses-scraping\\.venv\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:835\u001b[39m, in \u001b[36mGenerativeServiceClient.generate_content\u001b[39m\u001b[34m(self, request, model, contents, retry, timeout, metadata)\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_universe_domain()\n\u001b[32m    834\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m835\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[32m    843\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Kuliah\\Skripsi\\unhas-statistics-theses-scraping\\.venv\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Kuliah\\Skripsi\\unhas-statistics-theses-scraping\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    290\u001b[39m target = functools.partial(func, *args, **kwargs)\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Kuliah\\Skripsi\\unhas-statistics-theses-scraping\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m         result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n\u001b[32m    149\u001b[39m             warnings.warn(_ASYNC_RETRY_WARNING)\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Kuliah\\Skripsi\\unhas-statistics-theses-scraping\\.venv\\Lib\\site-packages\\google\\api_core\\timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    126\u001b[39m         remaining_timeout = \u001b[38;5;28mself\u001b[39m._timeout\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Kuliah\\Skripsi\\unhas-statistics-theses-scraping\\.venv\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:76\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(callable_)\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34merror_remapped_callable\u001b[39m(*args, **kwargs):\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     78\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Kuliah\\Skripsi\\unhas-statistics-theses-scraping\\.venv\\Lib\\site-packages\\grpc\\_interceptor.py:277\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable.__call__\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m    269\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    270\u001b[39m     request: Any,\n\u001b[32m   (...)\u001b[39m\u001b[32m    275\u001b[39m     compression: Optional[grpc.Compression] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    276\u001b[39m ) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     response, ignored_call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_with_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Kuliah\\Skripsi\\unhas-statistics-theses-scraping\\.venv\\Lib\\site-packages\\grpc\\_interceptor.py:329\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._with_call\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[32m    327\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _FailureOutcome(exception, sys.exc_info()[\u001b[32m2\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_interceptor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mintercept_unary_unary\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontinuation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient_call_details\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m call.result(), call\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Kuliah\\Skripsi\\unhas-statistics-theses-scraping\\.venv\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\transports\\grpc.py:79\u001b[39m, in \u001b[36m_LoggingClientInterceptor.intercept_unary_unary\u001b[39m\u001b[34m(self, continuation, client_call_details, request)\u001b[39m\n\u001b[32m     64\u001b[39m     grpc_request = {\n\u001b[32m     65\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpayload\u001b[39m\u001b[33m\"\u001b[39m: request_payload,\n\u001b[32m     66\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrequestMethod\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mgrpc\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     67\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(request_metadata),\n\u001b[32m     68\u001b[39m     }\n\u001b[32m     69\u001b[39m     _LOGGER.debug(\n\u001b[32m     70\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSending request for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclient_call_details.method\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     71\u001b[39m         extra={\n\u001b[32m   (...)\u001b[39m\u001b[32m     76\u001b[39m         },\n\u001b[32m     77\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m response = \u001b[43mcontinuation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient_call_details\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logging_enabled:  \u001b[38;5;66;03m# pragma: NO COVER\u001b[39;00m\n\u001b[32m     81\u001b[39m     response_metadata = response.trailing_metadata()\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Kuliah\\Skripsi\\unhas-statistics-theses-scraping\\.venv\\Lib\\site-packages\\grpc\\_interceptor.py:315\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001b[39m\u001b[34m(new_details, request)\u001b[39m\n\u001b[32m    306\u001b[39m (\n\u001b[32m    307\u001b[39m     new_method,\n\u001b[32m    308\u001b[39m     new_timeout,\n\u001b[32m   (...)\u001b[39m\u001b[32m    312\u001b[39m     new_compression,\n\u001b[32m    313\u001b[39m ) = _unwrap_client_call_details(new_details, client_call_details)\n\u001b[32m    314\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m     response, call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_thunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_method\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_credentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_wait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _UnaryOutcome(response, call)\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m rpc_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Kuliah\\Skripsi\\unhas-statistics-theses-scraping\\.venv\\Lib\\site-packages\\grpc\\_channel.py:1189\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable.with_call\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwith_call\u001b[39m(\n\u001b[32m   1181\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1182\u001b[39m     request: Any,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1187\u001b[39m     compression: Optional[grpc.Compression] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1188\u001b[39m ) -> Tuple[Any, grpc.Call]:\n\u001b[32m-> \u001b[39m\u001b[32m1189\u001b[39m     state, call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_blocking\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Kuliah\\Skripsi\\unhas-statistics-theses-scraping\\.venv\\Lib\\site-packages\\grpc\\_channel.py:1162\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._blocking\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m   1145\u001b[39m state.target = _common.decode(\u001b[38;5;28mself\u001b[39m._target)\n\u001b[32m   1146\u001b[39m call = \u001b[38;5;28mself\u001b[39m._channel.segregated_call(\n\u001b[32m   1147\u001b[39m     cygrpc.PropagationConstants.GRPC_PROPAGATE_DEFAULTS,\n\u001b[32m   1148\u001b[39m     \u001b[38;5;28mself\u001b[39m._method,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1160\u001b[39m     \u001b[38;5;28mself\u001b[39m._registered_call_handle,\n\u001b[32m   1161\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1162\u001b[39m event = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnext_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1163\u001b[39m _handle_event(event, state, \u001b[38;5;28mself\u001b[39m._response_deserializer)\n\u001b[32m   1164\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m state, call\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:388\u001b[39m, in \u001b[36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:211\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._next_call_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:205\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._next_call_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:97\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._latent_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:80\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._internal_latent_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:61\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._next\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "# 1. Set up your Google API Key\n",
    "load_dotenv()  # This loads variables from .env into the environment\n",
    "\n",
    "API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable not set. Please set your API key.\")\n",
    "\n",
    "genai.configure(api_key=API_KEY)\n",
    "\n",
    "# 2. Define filenames and batch size\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "INPUT_FILENAME = 'output/unhas_repository_20250730_004812.json'\n",
    "OUTPUT_FILENAME = f'output/unhas_repository_classified_{timestamp}.json'\n",
    "BATCH_SIZE = 20 \n",
    "\n",
    "# 3. Define the categories and instructions for the Gemini model\n",
    "CATEGORIES = {\n",
    "    \"Regresi\": \"Fokus pada pemodelan hubungan antara variabel yang bentuknya telah ditentukan sebelumnya (misalnya linear atau logistik). Tujuan utamanya adalah untuk inferensi, yaitu memahami dan mengukur seberapa besar pengaruh satu variabel terhadap variabel lainnya.\",\n",
    "    \"Regresi Nonparametrik\": \"Metode regresi yang digunakan ketika bentuk hubungan antara variabel tidak diasumsikan mengikuti model matematis tertentu. Pendekatan ini lebih fleksibel dan cocok untuk data dengan pola yang kompleks dan tidak linear. Contoh utamanya meliputi Regresi Spline, Regresi Kernel, dan Local Regression (LOESS).\",\n",
    "    \"Pengendalian Kualitas Statistika\": \"Fokus pada penggunaan alat statistik, terutama peta kendali (control chart), untuk memantau, mengendalikan, dan meningkatkan kualitas suatu proses yang sedang berjalan. Tujuannya adalah untuk mendeteksi variasi tak wajar secara visual agar proses tetap stabil dan outputnya konsisten sesuai standar.\",\n",
    "    \"Perancangan Percobaan\": \"Metodologi untuk merancang eksperimen dari awal secara efisien. Berbeda dengan Pengendalian Kualitas Statistika yang memantau proses, Perancangan Percobaan bertujuan untuk menguji dan membandingkan pengaruh berbagai perlakuan secara aktif dalam sebuah percobaan terkontrol (misalnya RAL, RAK) untuk menemukan pengaturan yang optimal.\",\n",
    "    \"Analisis Runtun Waktu\": \"Analisis data yang memiliki ketergantungan temporal, di mana urutan waktu pengamatan sangat penting. Tujuan utamanya adalah untuk memahami pola historis (tren, musiman) dan melakukan peramalan (forecasting) ke masa depan.\",\n",
    "    \"Machine Learning\": \"Bidang yang berfokus pada pengembangan algoritma untuk membuat prediksi atau klasifikasi seakurat mungkin dengan belajar dari data. ML lebih mengutamakan kemampuan prediktif daripada interpretasi model. Cakupannya luas, mulai dari metode seperti Support Vector Machine (SVM) dan aplikasi praktis seperti analisis sentimen, hingga model yang sangat kompleks dan seringkali bersifat black box.\",\n",
    "    \"Analisis Data Spasial\": \"Metode analisis khusus untuk data yang memiliki ketergantungan spasial, di mana lokasi geografis menjadi kunci. Fokusnya adalah memodelkan bagaimana nilai pada satu lokasi berhubungan dengan nilai di lokasi tetangganya. Ini seringkali melibatkan pengujian autokorelasi spasial (misalnya dengan Moran's I) dan penerapan model regresi yang disesuaikan untuk data spasial, seperti Geographically Weighted Regression (GWR), yang menghasilkan model lokal untuk setiap lokasi pengamatan.\",\n",
    "    \"Lainnya\": \"Kategori untuk topik penelitian skripsi yang tidak termasuk dalam klasifikasi fokus yang telah disebutkan di atas, seperti analisis survival, psikometri, atau bioinformatika.\"\n",
    "}\n",
    "\n",
    "def generate_classification_prompt(batch_items):\n",
    "    \"\"\"Generates the prompt for the Gemini API call.\"\"\"\n",
    "    category_list_str = \"\\n\".join([f\"- **{cat}**: {desc}\" for cat, desc in CATEGORIES.items()])\n",
    "    items_to_classify_str = json.dumps(batch_items, indent=2, ensure_ascii=False)\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert academic classifier specializing in statistics. Your task is to classify each research item into one of the following categories based on its title and abstract.\n",
    "\n",
    "    **Categories and Descriptions:**\n",
    "    {category_list_str}\n",
    "\n",
    "    **Instructions:**\n",
    "    1. Analyze the title and abstract for each item in the JSON array below.\n",
    "    2. For each item, determine the most fitting category from the list provided.\n",
    "    3. Your response MUST be a valid JSON object that maps each 'id' to its corresponding category name.\n",
    "    4. The category name MUST be one of these exact strings: {', '.join(CATEGORIES.keys())}.\n",
    "    5. Do NOT include any explanations, comments, or markdown formatting (like ```json) in your response.\n",
    "\n",
    "    **Research Items to Classify:**\n",
    "    {items_to_classify_str}\n",
    "\n",
    "    **Required Output Format (JSON object):**\n",
    "    {{\n",
    "      \"id_1\": \"CategoryName\",\n",
    "      \"id_2\": \"CategoryName\",\n",
    "      ...\n",
    "    }}\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def classify_theses():\n",
    "    \"\"\"Loads, classifies, and saves thesis data with improved robustness.\"\"\"\n",
    "    try:\n",
    "        with open(INPUT_FILENAME, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input file '{INPUT_FILENAME}' not found. Please run the scraper first.\")\n",
    "        return\n",
    "\n",
    "    model = genai.GenerativeModel('gemini-1.5-pro-latest')\n",
    "\n",
    "    tasks = []\n",
    "    task_id_counter = 0\n",
    "    for year, theses in data.items():\n",
    "        for title, details in theses.items():\n",
    "            if \"study_focus\" in details:\n",
    "                continue\n",
    "            \n",
    "            abstract = details.get(\"abstract\", \"\") or \"\"\n",
    "            if \"LIHAT DI FULL TEXT\" in abstract.upper():\n",
    "                abstract = \"\"\n",
    "            \n",
    "            tasks.append({\n",
    "                \"id\": f\"task_{task_id_counter}\",\n",
    "                \"title\": title,\n",
    "                \"abstract\": abstract,\n",
    "                \"original_object\": details\n",
    "            })\n",
    "            task_id_counter += 1\n",
    "\n",
    "    if not tasks:\n",
    "        print(\"✅ All items are already classified. No action needed.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(tasks)} items to classify. Starting process in batches of {BATCH_SIZE}...\")\n",
    "    \n",
    "    for i in range(0, len(tasks), BATCH_SIZE):\n",
    "        batch = tasks[i:i + BATCH_SIZE]\n",
    "        batch_input_for_prompt = [{\"id\": t[\"id\"], \"title\": t[\"title\"], \"abstract\": t[\"abstract\"]} for t in batch]\n",
    "        \n",
    "        print(f\"  - Processing batch {i//BATCH_SIZE + 1}/{(len(tasks) + BATCH_SIZE - 1)//BATCH_SIZE}...\")\n",
    "        \n",
    "        prompt = generate_classification_prompt(batch_input_for_prompt)\n",
    "        \n",
    "        classifications = {}\n",
    "        # --- IMPROVEMENT 1A: Added a retry mechanism ---\n",
    "        retries = 3\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                response = model.generate_content(prompt)\n",
    "                \n",
    "                # --- FIX: Clean and validate the response before parsing ---\n",
    "                # 1. Check if the response has text content.\n",
    "                if not response.text:\n",
    "                    raise ValueError(\"API returned an empty response.\")\n",
    "                \n",
    "                # 2. Clean potential markdown formatting.\n",
    "                cleaned_text = response.text.strip().replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "                \n",
    "                # 3. Parse the cleaned JSON.\n",
    "                classifications = json.loads(cleaned_text)\n",
    "                print(\"    - Batch successfully processed by API.\")\n",
    "                break # Exit retry loop on success\n",
    "            except (json.JSONDecodeError, ValueError) as e:\n",
    "                print(f\"    - Warning: API call or parsing failed on attempt {attempt + 1}. Error: {e}\")\n",
    "                # Log the problematic response for debugging\n",
    "                if 'response' in locals() and hasattr(response, 'text'):\n",
    "                    print(f\"    - Problematic API response text: '{response.text}'\")\n",
    "                if attempt < retries - 1:\n",
    "                    time.sleep(5) # Wait before retrying\n",
    "                else:\n",
    "                    print(f\"    - Error: Batch failed after {retries} attempts. Items will be marked 'Classification Failed'.\")\n",
    "            except Exception as e:\n",
    "                print(f\"    - Warning: An unexpected error occurred on attempt {attempt + 1}. Error: {e}\")\n",
    "                if attempt < retries - 1:\n",
    "                    time.sleep(5)\n",
    "                else:\n",
    "                    print(f\"    - Error: Batch failed after {retries} attempts. Items will be marked 'Classification Failed'.\")\n",
    "\n",
    "\n",
    "        # --- IMPROVEMENT 1B: Validate each classification ---\n",
    "        for task in batch:\n",
    "            task_id = task[\"id\"]\n",
    "            category = classifications.get(task_id) # Get the category from the API response\n",
    "            \n",
    "            if category and category in CATEGORIES:\n",
    "                task[\"original_object\"][\"study_focus\"] = category\n",
    "            else:\n",
    "                # If category is missing from response, or is not a valid category, mark it.\n",
    "                task[\"original_object\"][\"study_focus\"] = \"Classification Failed\"\n",
    "                if category: # Log if the category was invalid\n",
    "                    print(f\"    - Warning: Invalid category '{category}' for {task_id}. Defaulting to failed.\")\n",
    "\n",
    "    with open(OUTPUT_FILENAME, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "    print(f\"\\n✅ Classification complete! Results saved to '{OUTPUT_FILENAME}'.\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    classify_theses()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unhas-statistics-theses-scraping-3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
