{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9ef1123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from datetime import datetime\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4db36ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_repository():\n",
    "    \"\"\"\n",
    "    Scrapes thesis data from the UNHAS Statistics repository.\n",
    "    \"\"\"\n",
    "    # Automatically install and set up the ChromeDriver\n",
    "    service = ChromeService(ChromeDriverManager().install())\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\") # Optional: run in background\n",
    "    options.add_argument(\"--log-level=3\") # Suppress console logs\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "    base_url = \"https://repository.unhas.ac.id/view/divisions/statistika/\"\n",
    "    print(f\"Navigating to {base_url}...\")\n",
    "    driver.get(base_url)\n",
    "    time.sleep(3) \n",
    "\n",
    "    repository_data = {}\n",
    "\n",
    "    # Find all year links on the main page to avoid stale elements\n",
    "    year_elements = driver.find_elements(By.XPATH, \"/html/body/div[1]/div/div[2]/div/ul/li/a\")\n",
    "    year_links = [(elem.text, elem.get_attribute('href')) for elem in year_elements]\n",
    "\n",
    "    def get_element_text_or_none(driver, xpath):\n",
    "        \"\"\"Safely gets text from an element by its full XPath.\"\"\"\n",
    "        try:\n",
    "            return driver.find_element(By.XPATH, xpath).text.strip()\n",
    "        except NoSuchElementException:\n",
    "            return None\n",
    "    \n",
    "    # --- NEW, MORE ROBUST HELPER FUNCTION ---\n",
    "    def get_table_value_by_header(driver, header_text):\n",
    "        \"\"\"\n",
    "        Finds a table row by its header text and returns the value from the next cell.\n",
    "        This is more reliable than using a fixed row index.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # This XPath finds a <th> containing the header_text, then gets the text of the <td> next to it.\n",
    "            xpath = f\"//th[contains(text(), '{header_text}')]/following-sibling::td\"\n",
    "            return driver.find_element(By.XPATH, xpath).text.strip()\n",
    "        except NoSuchElementException:\n",
    "            return None\n",
    "\n",
    "    # Loop 1: Iterate through each year\n",
    "    for year_text, year_url in year_links:\n",
    "        print(f\"\\nProcessing Year: {year_text}\")\n",
    "        repository_data[year_text] = {}\n",
    "        driver.get(year_url)\n",
    "        time.sleep(2)\n",
    "\n",
    "        thesis_urls = []\n",
    "        thesis_index = 1\n",
    "        # Loop 2: Find all thesis links for the current year\n",
    "        while True:\n",
    "            try:\n",
    "                xpath = f\"/html/body/div[1]/div/div[2]/div[2]/p[{thesis_index}]/a\"\n",
    "                thesis_link_element = driver.find_element(By.XPATH, xpath)\n",
    "                thesis_urls.append(thesis_link_element.get_attribute('href'))\n",
    "                thesis_index += 1\n",
    "            except NoSuchElementException:\n",
    "                break # Exit loop when no more thesis links are found\n",
    "        \n",
    "        # Loop 3: Visit each thesis page and scrape data\n",
    "        for i, thesis_url in enumerate(thesis_urls):\n",
    "            driver.get(thesis_url)\n",
    "            time.sleep(1)\n",
    "\n",
    "            title = get_element_text_or_none(driver, '//*[@id=\"page-title\"]')\n",
    "            if not title:\n",
    "                print(f\"  - Skipping entry {i+1}/{len(thesis_urls)} (Title not found)\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"  - Scraping [{i+1}/{len(thesis_urls)}]: {title[:60]}...\")\n",
    "\n",
    "            # --- UPDATED SCRAPING LOGIC ---\n",
    "            # Scrape all required details using the new robust method for table data\n",
    "            thesis_details = {\n",
    "                \"author\": get_element_text_or_none(driver, \"/html/body/div[1]/div/div[2]/div/div[4]/p/span\"),\n",
    "                \"abstract\": get_element_text_or_none(driver, \"/html/body/div[1]/div/div[2]/div/div[4]/div[3]/p\"),\n",
    "                \"item_type\": get_table_value_by_header(driver, \"Item Type:\"),\n",
    "                \"date_deposited\": get_table_value_by_header(driver, \"Date Deposited:\"),\n",
    "                \"last_modified\": get_table_value_by_header(driver, \"Last Modified:\"),\n",
    "                \"url\": thesis_url\n",
    "            }\n",
    "            \n",
    "            repository_data[year_text][title] = thesis_details\n",
    "\n",
    "    # Save the final data structure to a JSON file\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_filename = f'output/unhas_repository_{timestamp}.json'\n",
    "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(repository_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"\\nâœ… Scraping complete. Data has been saved to '{output_filename}'.\")\n",
    "    driver.quit()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    scrape_repository()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f667057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# 1. Set up your Google API Key\n",
    "load_dotenv()  # This loads variables from .env into the environment\n",
    "\n",
    "API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable not set. Please set your API key.\")\n",
    "\n",
    "genai.configure(api_key=API_KEY)\n",
    "\n",
    "# 2. Define filenames and batch size\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "INPUT_FILENAME = 'output/unhas_repository_20250730_004812.json'\n",
    "OUTPUT_FILENAME = f'output/unhas_repository_classified_{timestamp}.json'\n",
    "BATCH_SIZE = 20 \n",
    "\n",
    "# 3. Define the categories and instructions for the Gemini model\n",
    "CATEGORIES = {\n",
    "    \"Regresi\":\"Fokus pada **inferensi statistik** untuk memahami dan mengukur hubungan antar variabel menggunakan model dengan **bentuk fungsional yang telah ditentukan** (misalnya, linear, logistik). Tujuan utamanya adalah menjelaskan *seberapa besar* pengaruh satu variabel terhadap variabel lain.\",\n",
    "    \"Regresi Nonparametrik\":\"Fokus pada pemodelan hubungan antar variabel **TANPA asumsi bentuk fungsional tertentu**. Metode ini sangat fleksibel dan digunakan ketika pola data kompleks, non-linear, dan tidak diketahui sebelumnya. Tujuannya adalah membiarkan data 'berbicara' untuk membentuk modelnya sendiri.\",\n",
    "    \"Pengendalian Kualitas Statistika\":\"Fokus pada **pemantauan (monitoring) proses yang sedang berjalan** untuk memastikan stabilitas dan konsistensi output. Alat utamanya adalah **peta kendali (control chart)** untuk mendeteksi variasi yang tidak wajar secara visual dan menjaga proses tetap dalam spesifikasi.\",\n",
    "    \"Perancangan Percobaan\":\"Fokus pada **perancangan eksperimen secara proaktif SEBELUM data dikumpulkan**. Tujuannya adalah untuk secara efisien membandingkan efek dari berbagai **perlakuan (treatments)** melalui intervensi aktif untuk menemukan pengaturan atau kondisi yang paling optimal.\",\n",
    "    \"Analisis Runtun Waktu\":\"Analisis data yang variabel utamanya adalah **waktu**. Metode ini secara khusus menangani data dengan **ketergantungan temporal** (nilai saat ini dipengaruhi oleh nilai sebelumnya). Tujuan utamanya adalah memahami pola historis dan melakukan **peramalan (forecasting)**.\",\n",
    "    \"Machine Learning\":\"Fokus utama pada **akurasi prediksi**. Tujuannya adalah membangun algoritma yang dapat belajar dari data untuk membuat prediksi atau klasifikasi seakurat mungkin, seringkali **mengorbankan interpretasi model** demi performa prediktif yang superior.\",\n",
    "    \"Analisis Data Spasial\":\"Analisis data yang variabel utamanya adalah **lokasi geografis**. Metode ini secara khusus menangani data dengan **ketergantungan spasial** (nilai di satu lokasi dipengaruhi oleh nilai di lokasi tetangganya). Fokus utamanya adalah pemetaan dan pemodelan **autokorelasi spasial**.\",\n",
    "    \"Analisis Survival\":\"Metode statistik khusus untuk menganalisis data **'waktu-ke-kejadian' (time-to-event)**. Fokusnya adalah memodelkan waktu hingga suatu peristiwa terjadi dan menangani **data tersensor (censored data)**, di mana peristiwa tersebut tidak diamati untuk semua subjek.\",\n",
    "    \"Ekonometrika dan Manajemen Risiko\":\"Aplikasi statistik khusus pada **data keuangan dan ekonomi** untuk mengukur dan mengelola risiko. Fokus utamanya adalah kuantifikasi risiko investasi melalui metrik seperti **Value at Risk (VaR) dan CVaR**, pemodelan portofolio, dan analisis dependensi aset.\",\n",
    "    \"Lainnya\":\"Kategori untuk metodologi statistik yang tidak memiliki karakteristik unik dari kategori lain yang telah disebutkan. Contohnya meliputi **psikometri, bioinformatika, atau analisis data kategorik murni**.\"\n",
    "}\n",
    "\n",
    "def generate_classification_prompt(batch_items):\n",
    "    \"\"\"Generates the prompt for the Gemini API call.\"\"\"\n",
    "    category_list_str = \"\\n\".join([f\"- **{cat}**: {desc}\" for cat, desc in CATEGORIES.items()])\n",
    "    items_to_classify_str = json.dumps(batch_items, indent=2, ensure_ascii=False)\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert academic classifier specializing in statistics. Your task is to classify each research item into one of the following categories based on its title and abstract.\n",
    "\n",
    "    **Categories and Descriptions:**\n",
    "    {category_list_str}\n",
    "\n",
    "    **Instructions:**\n",
    "    1. Analyze the title and abstract for each item in the JSON array below.\n",
    "    2. For each item, determine the most fitting category from the list provided.\n",
    "    3. Your response MUST be a valid JSON object that maps each 'id' to its corresponding category name.\n",
    "    4. The category name MUST be one of these exact strings: {', '.join(CATEGORIES.keys())}.\n",
    "    5. Do NOT include any explanations, comments, or markdown formatting (like ```json) in your response.\n",
    "\n",
    "    **Research Items to Classify:**\n",
    "    {items_to_classify_str}\n",
    "\n",
    "    **Required Output Format (JSON object):**\n",
    "    {{\n",
    "      \"id_1\": \"CategoryName\",\n",
    "      \"id_2\": \"CategoryName\",\n",
    "      ...\n",
    "    }}\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def classify_theses():\n",
    "    \"\"\"Loads, classifies, and saves thesis data with improved robustness.\"\"\"\n",
    "    try:\n",
    "        with open(INPUT_FILENAME, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input file '{INPUT_FILENAME}' not found. Please run the scraper first.\")\n",
    "        return\n",
    "\n",
    "    model = genai.GenerativeModel('gemini-2.5-pro')\n",
    "\n",
    "    tasks = []\n",
    "    task_id_counter = 0\n",
    "    for year, theses in data.items():\n",
    "        for title, details in theses.items():\n",
    "            if \"study_focus\" in details:\n",
    "                continue\n",
    "            \n",
    "            abstract = details.get(\"abstract\", \"\") or \"\"\n",
    "            if \"LIHAT DI FULL TEXT\" in abstract.upper():\n",
    "                abstract = \"\"\n",
    "            \n",
    "            tasks.append({\n",
    "                \"id\": f\"task_{task_id_counter}\",\n",
    "                \"title\": title,\n",
    "                \"abstract\": abstract,\n",
    "                \"original_object\": details\n",
    "            })\n",
    "            task_id_counter += 1\n",
    "\n",
    "    if not tasks:\n",
    "        print(\"âœ… All items are already classified. No action needed.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(tasks)} items to classify. Starting process in batches of {BATCH_SIZE}...\")\n",
    "    \n",
    "    for i in range(0, len(tasks), BATCH_SIZE):\n",
    "        batch = tasks[i:i + BATCH_SIZE]\n",
    "        batch_input_for_prompt = [{\"id\": t[\"id\"], \"title\": t[\"title\"], \"abstract\": t[\"abstract\"]} for t in batch]\n",
    "        \n",
    "        print(f\"  - Processing batch {i//BATCH_SIZE + 1}/{(len(tasks) + BATCH_SIZE - 1)//BATCH_SIZE}...\")\n",
    "        \n",
    "        prompt = generate_classification_prompt(batch_input_for_prompt)\n",
    "        \n",
    "        classifications = {}\n",
    "        # --- IMPROVEMENT 1A: Added a retry mechanism ---\n",
    "        retries = 3\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                response = model.generate_content(prompt)\n",
    "                \n",
    "                # --- FIX: Clean and validate the response before parsing ---\n",
    "                # 1. Check if the response has text content.\n",
    "                if not response.text:\n",
    "                    raise ValueError(\"API returned an empty response.\")\n",
    "                \n",
    "                # 2. Clean potential markdown formatting.\n",
    "                cleaned_text = response.text.strip().replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "                \n",
    "                # 3. Parse the cleaned JSON.\n",
    "                classifications = json.loads(cleaned_text)\n",
    "                print(\"    - Batch successfully processed by API.\")\n",
    "                break # Exit retry loop on success\n",
    "            except (json.JSONDecodeError, ValueError) as e:\n",
    "                print(f\"    - Warning: API call or parsing failed on attempt {attempt + 1}. Error: {e}\")\n",
    "                # Log the problematic response for debugging\n",
    "                if 'response' in locals() and hasattr(response, 'text'):\n",
    "                    print(f\"    - Problematic API response text: '{response.text}'\")\n",
    "                if attempt < retries - 1:\n",
    "                    time.sleep(5) # Wait before retrying\n",
    "                else:\n",
    "                    print(f\"    - Error: Batch failed after {retries} attempts. Items will be marked 'Classification Failed'.\")\n",
    "            except Exception as e:\n",
    "                print(f\"    - Warning: An unexpected error occurred on attempt {attempt + 1}. Error: {e}\")\n",
    "                if attempt < retries - 1:\n",
    "                    time.sleep(5)\n",
    "                else:\n",
    "                    print(f\"    - Error: Batch failed after {retries} attempts. Items will be marked 'Classification Failed'.\")\n",
    "\n",
    "\n",
    "        # --- IMPROVEMENT 1B: Validate each classification ---\n",
    "        for task in batch:\n",
    "            task_id = task[\"id\"]\n",
    "            category = classifications.get(task_id) # Get the category from the API response\n",
    "            \n",
    "            if category and category in CATEGORIES:\n",
    "                task[\"original_object\"][\"study_focus\"] = category\n",
    "            else:\n",
    "                # If category is missing from response, or is not a valid category, mark it.\n",
    "                task[\"original_object\"][\"study_focus\"] = \"Classification Failed\"\n",
    "                if category: # Log if the category was invalid\n",
    "                    print(f\"    - Warning: Invalid category '{category}' for {task_id}. Defaulting to failed.\")\n",
    "\n",
    "    with open(OUTPUT_FILENAME, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "    print(f\"\\nâœ… Classification complete! Results saved to '{OUTPUT_FILENAME}'.\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    classify_theses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95324e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUTPUT_FILENAME, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "rows = []\n",
    "for year, theses in data.items():\n",
    "    for title, details in theses.items():\n",
    "        row = {'year': year, 'title': title}\n",
    "        row.update(details)\n",
    "        rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "df.to_excel(f'output/unhas_repository_classified_{timestamp}.xlsx', index=False)\n",
    "print(f\"âœ… Data exported to 'output/unhas_repository_classified_{timestamp}.xlsx'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0d1510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_repository_data(input_path):\n",
    "    \"\"\"\n",
    "    Reads a nested JSON repository file, flattens it, and extracts\n",
    "    only the title, abstract, and study focus for each entry.\n",
    "\n",
    "    Args:\n",
    "        input_path (str): The path to the source JSON file.\n",
    "\n",
    "    Returns:\n",
    "        str: The path to the newly created simplified JSON file.\n",
    "    \"\"\"\n",
    "    # Define output directory and create a timestamped filename\n",
    "    output_dir = os.path.dirname(input_path)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_filename = f'unhas_repository_simplified_{timestamp}.json'\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "    \n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    simplified_list = []\n",
    "    # Iterate through the top-level keys (e.g., '2002', 'NULL')\n",
    "    for year_key in data:\n",
    "        # Iterate through each paper's title and its details\n",
    "        for title, details in data[year_key].items():\n",
    "            new_entry = {\n",
    "                'title': title,\n",
    "                'abstract': details.get('abstract', 'Not Available'),\n",
    "                'study_focus': details.get('study_focus', 'Not Available')\n",
    "            }\n",
    "            simplified_list.append(new_entry)\n",
    "\n",
    "    # Write the new flat list to the output file\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(simplified_list, f, indent=4, ensure_ascii=False)\n",
    "        \n",
    "    return output_path\n",
    "\n",
    "# --- Execution ---\n",
    "if __name__ == '__main__':\n",
    "    # Specify the path to your source file\n",
    "    input_file = 'output/20250730/unhas_repository_classified_20250730_114427.json'\n",
    "    \n",
    "    # Run the function and print the result\n",
    "    new_file_path = simplify_repository_data(input_file)\n",
    "    print(f\"âœ… Successfully created simplified JSON file at: {new_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unhas-statistics-theses-scraping-3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
